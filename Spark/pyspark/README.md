* [Generators and PySpark](https://www.waitingforcode.com/pyspark/generators-pyspark/read)
* [Reduce your worries: using ‘reduce’ with PySpark](https://towardsdatascience.com/reduce-your-worries-using-reduce-with-pyspark-642106d6ae50)
* [PySpark transform() Function with Example](https://sparkbyexamples.com/pyspark/pyspark-transform-function/)
* [Data Transformation in PySpark](https://towardsdatascience.com/data-transformation-in-pyspark-6a88a6193d92)
* [Spark Transformation and Action: A Deep Dive](https://medium.com/codex/spark-transformation-and-action-a-deep-dive-f351bce88086)
* [Apply a transformation to multiple columns PySpark dataframe](https://www.geeksforgeeks.org/apply-a-transformation-to-multiple-columns-pyspark-dataframe/)
* [PySpark Transforms](https://developer.ascend.io/docs/pyspark-transforms)
* [Data Frame Operations – Basic Transformations such as filtering, aggregations, joins etc](https://kaizen.itversity.com/courses/hdpcsd-hdp-certified-spark-developer-hdpcsd-python/lessons/hdpcsd-apache-spark-2-data-frames-and-spark-sql-python/topic/hdpcsd-data-frame-operations-basic-transformations-such-as-filtering-aggregations-joins-etc-python/)
* [Spark SQL - DataFrame - select - transformation or action?](https://stackoverflow.com/questions/46582466/spark-sql-dataframe-select-transformation-or-action)
* [Transforming PySpark DataFrames](https://hackersandslackers.com/transforming-pyspark-dataframes/)
* [pyspark.sql.functions.transform](https://spark.apache.org/docs/3.1.1/api/python/reference/api/pyspark.sql.functions.transform.html)
* [Best Practices](https://spark.apache.org/docs/latest/api/python/user_guide/pandas_on_spark/best_practices.html)
* [How to Chain Functions in Pyspark with Pipe](https://towardsdatascience.com/how-to-chain-functions-in-pyspark-with-pandas-pipe-1915ed7ebc34)
* [convert columns of pyspark data frame to lowercase](https://stackoverflow.com/questions/43005744/convert-columns-of-pyspark-data-frame-to-lowercase)
* [Operations on Multiple Columns at Once](https://haya-toumy.gitbook.io/spark-notes/pyspark/pyspark/operations-on-multiple-columns-at-once)
* [Add column sum as new column in PySpark dataframe](https://stackoverflow.com/questions/31955309/add-column-sum-as-new-column-in-pyspark-dataframe)
* [pyspark.sql.DataFrame.approxQuantile](https://api-docs.databricks.com/python/pyspark/latest/pyspark.sql/api/pyspark.sql.DataFrame.approxQuantile.html)
* [Remove duplicates from PySpark array column](https://stackoverflow.com/questions/54185710/remove-duplicates-from-pyspark-array-column)
* [Statistical and Mathematical Functions with DataFrames in Apache Spark](https://www.databricks.com/blog/2015/06/02/statistical-and-mathematical-functions-with-dataframes-in-spark.html)
* [Advanced Pyspark for Exploratory Data Analysis](https://www.kaggle.com/code/tientd95/advanced-pyspark-for-exploratory-data-analysis)
* [class pyspark.ml.stat.Summarizer](https://spark.apache.org/docs/latest/api/python/reference/api/pyspark.ml.stat.Summarizer.html) 
* [Mode of grouped data in (py)Spark](https://stackoverflow.com/questions/36654162/mode-of-grouped-data-in-pyspark)
* [Calculate the mode of a PySpark DataFrame column?](https://stackoverflow.com/questions/34607091/calculate-the-mode-of-a-pyspark-dataframe-column)
* [pyspark.sql.functions.mode](https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/api/pyspark.sql.functions.mode.html)
* [PySpark Pivot and Unpivot DataFrame](https://sparkbyexamples.com/pyspark/pyspark-pivot-and-unpivot-dataframe/?expand_article=1)
* [Pyspark: GroupBy and Aggregate Functions](https://hendra-herviawan.github.io/pyspark-groupby-and-aggregate-functions.html)
* [PySpark Groupby Agg (aggregate) – Explained](https://sparkbyexamples.com/pyspark/pyspark-groupby-agg-aggregate-explained/)
* [PySpark Groupby Explained with Example](https://sparkbyexamples.com/pyspark/pyspark-groupby-explained-with-example/)
* [3 Ways To Aggregate Data In PySpark](https://towardsdatascience.com/3-ways-to-aggregate-data-in-pyspark-72209197c90)
* [PySpark fillna() & fill() – Replace NULL/None Values](https://sparkbyexamples.com/pyspark/pyspark-fillna-fill-replace-null-values/?expand_article=1)
* [PySpark StructType & StructField Explained with Examples](https://sparkbyexamples.com/pyspark/pyspark-structtype-and-structfield/)
* [PySpark Select Nested struct Columns](https://sparkbyexamples.com/pyspark/pyspark-select-nested-struct-columns/)
* [How to use struct() function in PySpark Azure Databricks?](https://azurelib.com/how-to-use-struct-function-in-pyspark-azure-databricks/)
* [PySpark ArrayType Column With Examples](https://sparkbyexamples.com/pyspark/pyspark-arraytype-column-with-examples/)
* [ArrayType Columns](https://mungingdata.com/pyspark/array-arraytype-list/)
* [add days to date column](https://stackoverflow.com/questions/50703284/how-to-add-ten-days-to-existing-date-column-in-pyspark)
* [array column contains](https://sparkbyexamples.com/spark/spark-array_contains-example/)
* [create column grouping with map type](https://stackoverflow.com/questions/45532183/create-dataframe-grouping-columns-in-map-type)
* [PySpark – max()](https://linuxhint.com/max-pyspark/)
* [Best way to get the max value in a Spark dataframe column](https://intellipaat.com/community/4448/best-way-to-get-the-max-value-in-a-spark-dataframe-column)
* [Spark Performance Tuning & Best Practices](https://sparkbyexamples.com/spark/spark-performance-tuning/)
* [PySpark – Coding Standards & Best Practices](https://blogs.perficient.com/2022/09/30/pyspark-coding-standards-best-practices/)
* [Best Practices and Performance Tuning Activities for PySpark](https://www.analyticsvidhya.com/blog/2021/08/best-practices-and-performance-tuning-activities-for-pyspark/)
* [Best Practices for Running PySpark](https://www.databricks.com/session/best-practices-for-running-pyspark)
* [PySpark Code review checklist and best practices](https://www.linkedin.com/pulse/pyspark-code-review-checklist-best-practices-cheruvu-he-him-/)
* [Use checkpoint](https://spark.apache.org/docs/latest/api/python/user_guide/pandas_on_spark/best_practices.html)
* [Deep Dive: Apache Spark Memory Management](https://www.databricks.com/session/deep-dive-apache-spark-memory-management)
* [Spark Memory Management](https://community.cloudera.com/t5/Community-Articles/Spark-Memory-Management/ta-p/317794#:~:text=Spark%20Memory%20is%20the%20memory,storage%20memory%20of%20this%20segment.)
* [Apache Spark Memory Management](https://medium.com/analytics-vidhya/apache-spark-memory-management-49682ded3d42)
* [Memory Profiling in PySpark](https://www.databricks.com/blog/2022/11/30/memory-profiling-pyspark.html)
* [pyspark.sql.DataFrame.subtract](https://spark.apache.org/docs/3.2.1/api/python/reference/api/pyspark.sql.DataFrame.subtract.html)
* [A Developer’s View Into Spark’s Memory Model](https://www.databricks.com/session/a-developers-view-into-sparks-memory-model)
* [Apache Spark Memory Management](https://medium.com/analytics-vidhya/apache-spark-memory-management-49682ded3d42)
* [A PySpark Example for Dealing with Larger than Memory Datasets](https://towardsdatascience.com/a-pyspark-example-for-dealing-with-larger-than-memory-datasets-70dbc82b0e98)
* [Understanding core concepts in Apache Spark](https://medium.com/@georgiadeaconu/understanding-core-concepts-in-apache-spark-73dce67a5f73)
* [createOrReplaceTempView](https://spark.apache.org/docs/3.1.3/api/python/reference/api/pyspark.sql.DataFrame.createOrReplaceTempView.html)
* [PySpark createOrReplaceTempView() Explained](https://sparkbyexamples.com/pyspark/pyspark-createorreplacetempview/)
* [How does createOrReplaceTempView work in Spark?](https://stackoverflow.com/questions/44011846/how-does-createorreplacetempview-work-in-spark)
* [DataFrame Checkpoint](https://spark.apache.org/docs/3.1.1/api/python/reference/api/pyspark.sql.DataFrame.checkpoint.html#:~:text=checkpoint,-DataFrame.&text=Returns%20a%20checkpointed%20version%20of,the%20plan%20may%20grow%20exponentially.)
* [setCheckpointDir](https://spark.apache.org/docs/latest/api/python/reference/api/pyspark.SparkContext.setCheckpointDir.html)
* [Dataframe Checkpoint Example Pyspark](https://stackoverflow.com/questions/67916597/dataframe-checkpoint-example-pyspark)
* [Append 2 dataframes with pyspark](https://www.learneasysteps.com/steps-to-append-2-dataframes-in-pyspark/)
* [PySpark Groupby Explained with Example](https://sparkbyexamples.com/pyspark/pyspark-groupby-explained-with-example/)
* [How to Pivot and Unpivot a Spark Data Frame](https://sparkbyexamples.com/spark/how-to-pivot-table-and-unpivot-a-spark-dataframe/)
* [PySpark How to Filter Rows with NULL Values](https://sparkbyexamples.com/pyspark/pyspark-filter-rows-with-null-values/
* [Simplify data conversion from Spark to PyTorch](https://www.databricks.com/notebooks/simple-aws/petastorm-spark-converter-pytorch.html)
* [How to Parallelize and Distribute Collection in PySpark](https://medium.com/@nutanbhogendrasharma/how-to-parallelize-and-distribute-collection-in-pyspark-5f58c86d7582)
* [pyspark equivalence of `df.loc`?](https://stackoverflow.com/questions/50311732/pyspark-equivalence-of-df-loc)
* [Filtering a PySpark DataFrame using isin by exclusion](https://www.geeksforgeeks.org/filtering-a-pyspark-dataframe-using-isin-by-exclusion/)
* [How to filter based on array value in PySpark?](https://newbedev.com/how-to-filter-based-on-array-value-in-pyspark)
* [TYPECAST INTEGER TO STRING AND STRING TO INTEGER IN PYSPARK](https://www.datasciencemadesimple.com/typecast-integer-to-string-and-string-to-integer-in-pyspark/)
* [Supported Data Types](https://spark.apache.org/docs/latest/sql-ref-datatypes.html)
* [Cast Column Type With Examples](https://sparkbyexamples.com/pyspark/pyspark-cast-column-type/)
* [Data exploration with pyspark](https://runawayhorse001.github.io/LearningApacheSpark/exploration.html)
* [Convert continuous variables to categorical variables](https://notebook.community/MingChen0919/learning-apache-spark/notebooks/02-data-manipulation/.ipynb_checkpoints/2.3-continuous-variable-to-categorical-variable-checkpoint)
* [How to transform a categorical variable in Spark into a set of columns coded as {0,1}?](https://stackoverflow.com/questions/30104726/how-to-transform-a-categorical-variable-in-spark-into-a-set-of-columns-coded-as)
* [Converting one column from string to float_double](https://community.databricks.com/s/question/0D53f00001HKHe9CAH/pyspark-dataframe-converting-one-column-from-string-to-floatdouble)
* [Compare two dataframes Pyspark](https://stackoverflow.com/questions/60279160/compare-two-dataframes-pyspark/60284807)
* [How to find count of Null and Nan values for each column in a PySpark dataframe efficiently?](https://stackoverflow.com/questions/44627386/how-to-find-count-of-null-and-nan-values-for-each-column-in-a-pyspark-dataframe)
* [Merge two DataFrames in PySpark](https://www.geeksforgeeks.org/merge-two-dataframes-in-pyspark/)
* [pyspark join types](https://sparkbyexamples.com/pyspark/pyspark-join-explained-with-examples/#pyspark-join-types)
* [pyspark: merge (outer-join) two data frames](https://stackoverflow.com/questions/38063657/pyspark-merge-outer-join-two-data-frames)
* [Merge two DataFrames in PySpark](https://www.geeksforgeeks.org/merge-two-dataframes-in-pyspark/)
* [dates with pyspark](https://mungingdata.com/apache-spark/dates-times/)
* [Working with dates and times in Spark](https://mrpowers.medium.com/working-with-dates-and-times-in-spark-491a9747a1d2)
* [PySpark Date Functions](https://sqlandhadoop.com/pyspark-date-functions/)
* [PySpark How to Filter Rows with NULL Values](https://sparkbyexamples.com/pyspark/pyspark-filter-rows-with-null-values/)
* [Sparkit-learn](https://github.com/lensacom/sparkit-learn)
* [pyspark split dataframe by rows](https://www.grepper.com/answers/371707/pyspark+split+dataframe+by+rows)
* [Fill NAs with mode of column based on aggregation of other columns](https://stackoverflow.com/questions/65928710/pyspark-fill-nas-with-mode-of-column-based-on-aggregation-of-other-columns)
* [PySpark -> interpolate values in one column](https://stackoverflow.com/questions/57114835/pyspark-interpolate-values-in-one-column)
* [HandySpark: bringing pandas-like capabilities to Spark DataFrames](https://towardsdatascience.com/handyspark-bringing-pandas-like-capabilities-to-spark-dataframes-5f1bcea9039e)
* [The Most Complete Guide to pySpark DataFrames](https://towardsdatascience.com/the-most-complete-guide-to-pyspark-dataframes-2702c343b2e8)
* [Apache Spark Performance Boosting](https://towardsdatascience.com/apache-spark-performance-boosting-e072a3ec1179)
* [Using PySpark Dataframe as in Python](https://medium.com/@zhiqiangzhong/using-pyspark-dataframe-as-python-dataframe-2959c2a085e)
* [Working with PySpark DataFrames](https://medium.com/@aniketmohan/working-with-pyspark-dataframes-456c7bdb5b5c)
* [Pyspark dataframe: Summing over a column while grouping over another](https://stackoverflow.com/questions/33961899/pyspark-dataframe-summing-over-a-column-while-grouping-over-another)
* [PySpark Add a New Column to DataFrame](https://sparkbyexamples.com/pyspark/pyspark-add-new-column-to-dataframe/)
* [PySpark Groupby Explained with Example](https://sparkbyexamples.com/pyspark/pyspark-groupby-explained-with-example/)
* [Pyspark: GroupBy and Aggregate Functions](https://hendra-herviawan.github.io/pyspark-groupby-and-aggregate-functions.html)
* [Extracting, transforming and selecting features](https://spark.apache.org/docs/3.0.0-preview/ml-features.html)
* [Basic data preparation in Pyspark — Capping, Normalizing and Scaling](https://medium.com/@connectwithghosh/basic-data-preparation-in-pyspark-capping-normalizing-and-scaling-252ee7acba7d)
* [GET DATA TYPE OF COLUMN IN PYSPARK (SINGLE & MULTIPLE COLUMNS)](https://www.datasciencemadesimple.com/data-type-of-column-in-pyspark-single-multiple-columns/)
* [get datatype of column using pyspark](https://stackoverflow.com/questions/45033315/get-datatype-of-column-using-pyspark)
* [PySpark: withColumn() with two conditions and three outcomes](https://stackoverflow.com/questions/40161879/pyspark-withcolumn-with-two-conditions-and-three-outcomes)
* [ADD LEADING ZEROS TO THE COLUMN IN PYSPARK](https://www.datasciencemadesimple.com/add-leading-zeros-to-the-column-in-pyspark/)
* [PySpark Read and Write Parquet File](https://sparkbyexamples.com/pyspark/pyspark-read-and-write-parquet-file/)
* [PySpark Replace Column Values in DataFrame](https://sparkbyexamples.com/pyspark/pyspark-replace-column-values/)
* [PySpark When Otherwise | SQL Case When Usage](https://sparkbyexamples.com/pyspark/pyspark-when-otherwise/)
* [PySpark Where Filter Function | Multiple Conditions](https://sparkbyexamples.com/pyspark/pyspark-where-filter/)
* [Show distinct column values in pyspark dataframe](https://stackoverflow.com/questions/39383557/show-distinct-column-values-in-pyspark-dataframe)
* [PySpark orderBy() and sort() explained](https://sparkbyexamples.com/pyspark/pyspark-orderby-and-sort-explained/)
* [PySpark fillna() & fill() – Replace NULL/None Values](https://sparkbyexamples.com/pyspark/pyspark-fillna-fill-replace-null-values/)
* [PySpark Concatenate Columns](https://sparkbyexamples.com/pyspark/pyspark-concatenate-columns/)
* [ML Pipelines](https://spark.apache.org/docs/latest/ml-pipeline.html)
* [Overview: estimators, transformers and pipelines - spark.ml](https://spark.apache.org/docs/1.6.1/ml-guide.html)
* [pyspark ml pipeline](https://www.programcreek.com/python/example/105437/pyspark.ml.Pipeline)
* []()
* []()
* [Plot Data from Apache Spark in Python/v3](https://plotly.com/python/v3/apache-spark/)
* [PySpark apply function to column](https://sqlandhadoop.com/pyspark-apply-function-to-column/)
* [PySpark SQL expr() (Expression ) Function](https://sparkbyexamples.com/pyspark/pyspark-sql-expr-expression-function/)
* [PySpark lit() – Add Literal or Constant to DataFrame](https://sparkbyexamples.com/pyspark/pyspark-lit-add-literal-constant/)
* [PySpark map() Transformation](https://sparkbyexamples.com/pyspark/pyspark-map-transformation/)
* [PySpark flatMap() Transformation](https://sparkbyexamples.com/pyspark/pyspark-flatmap-transformation/)
* [PySpark MapType (Dict) Usage with Examples](https://sparkbyexamples.com/pyspark/pyspark-maptype-dict-examples/)
* [PySpark Convert DataFrame Columns to MapType (Dict)](https://sparkbyexamples.com/pyspark/pyspark-convert-dataframe-columns-to-maptype-dict/#:~:text=Solution%3A%20PySpark%20SQL%20function%20create_map,and%20returns%20a%20MapType%20column.)
* [Pyspark apply function to column value if condition is met [duplicate]](https://stackoverflow.com/questions/56309596/pyspark-apply-function-to-column-value-if-condition-is-met)
* [Pyspark: Pass multiple columns in UDF](https://stackoverflow.com/questions/42540169/pyspark-pass-multiple-columns-in-udf)
* [How To Select Rows From PySpark DataFrames Based on Column Values](https://towardsdatascience.com/select-rows-pyspark-df-based-on-column-values-3146afe4dee3)
* [When Otherwise | SQL Case When Usage](https://sparkbyexamples.com/pyspark/pyspark-when-otherwise/)
* [PySpark & Plotly](https://nnovakova.github.io/pyspark-plotly/)
* [orderBy() and sort() explained](https://sparkbyexamples.com/pyspark/pyspark-orderby-and-sort-explained/)
* [cartesian product (filling spaces](https://stackoverflow.com/questions/40728618/creating-each-row-for-each-item-for-a-user-in-spark-dataframe)
* [The Most Complete Guide to pySpark DataFrames](https://towardsdatascience.com/the-most-complete-guide-to-pyspark-dataframes-2702c343b2e8)
* [9 most useful functions for PySpark DataFrame](https://www.analyticsvidhya.com/blog/2021/05/9-most-useful-functions-for-pyspark-dataframe/)
* [Pyspark Data Manipulation Tutorial](https://towardsdatascience.com/pyspark-data-manipulation-tutorial-8c62652f35fa)
* [PySpark Where Filter Function | Multiple Conditions](https://sparkbyexamples.com/pyspark/pyspark-where-filter/)
* [PySpark split() Column into Multiple Columns](https://sparkbyexamples.com/pyspark/pyspark-split-dataframe-column-into-multiple-columns/)
* [Sum a column in dataframe and return results as int](https://stackoverflow.com/questions/47812526/pyspark-sum-a-column-in-dataframe-and-return-results-as-int)
* [Extracting, transforming and selecting features](https://spark.apache.org/docs/latest/ml-features)
* [Feature Engineering in pyspark](https://dhiraj-p-rai.medium.com/essentials-of-feature-engineering-in-pyspark-part-i-76a57680a85)
* [Get specific row from PySpark dataframe](https://www.geeksforgeeks.org/get-specific-row-from-pyspark-dataframe/)

drop column by name
```python
df.drop(col("firstname"))
```
drop rows with null values
from [stackOverflow](https://sparkbyexamples.com/pyspark/pyspark-drop-rows-with-null-values/)
```python
df.na.drop("any").show(false)
```

list of unique values from column:
```python
unique_list = [i.variable for i in df.select('variable').distinct().collect()]
```


filter df with elements of dict:
```python
filtered_df = df.filter(F.col('col_name').isin(dict_name['dict_element_name']))
```

filter df with elements of list:
```python
filtered_df = df.filter(F.col('col_name').isin(list_name))
```
or
```python
filtered_df.where((filtered_df.col_name).isin(list_name))
```


Join 2 df's:
guide is [here](https://sparkbyexamples.com/pyspark/pyspark-join-explained-with-examples/)
```python
df_joined = df_orig.join(df_to_join, on=join_cols, how='left')
```


renaming a column:
```python
df = df.withColumnRenamed(column_name, new_column_name)
```

copy a column:
```python
df = df.withColumn("new_column_name", df["column_name"])
```

change columns data type:
```python
df = df.withColumn("column_name", df["column_name"].cast(IntegerType()))
```

select columns:
```python
df_selected_cols = df_original.select('cols_to_select')
```

get columns data types:
```python
df.dtypes
```

get elements type on list:
```python
from collections import Counter

Counter([type(value) for value in lista])
```

delete NoneType element from list:
```python
res = list(filter(None, lista))
```

convert pyspark DF to Pandas DF:
```python
pandas_df = pyspark_df.toPandas()
```

convert Pandas DF to Pyspark DF:
```python
sparkDF=spark.createDataFrame(pandasDF) 
```

add a column with value depending of whether the value is in list or not

```python
prueba = (
    df
    .withColumn("new_col_name",sf.when(sf.col(col_name).isin(list_name),"value to assign").otherwise("value if not true") )
) 
```

or

```python
prueba = (
    df
    .withColumn("new_col_name",sf.when(sf.col(col_name).isin(list_name),"value to assign").otherwise(sf.col(col_name) )
) 
```

Filter by column value
```python
df.filter(sf.col('column_name')==value_looking_for)
```

from [stackoverflow](https://stackoverflow.com/questions/40421845/pyspark-dataframe-filter-or-include-based-on-list)
```python
def filter_spark_dataframe_by_list(df, column_name, filter_list):
    """ Returns subset of df where df[column_name] is in filter_list """
    spark = SparkSession.builder.getOrCreate()
    filter_df = spark.createDataFrame(filter_list, df.schema[column_name].dataType)
    return df.join(filter_df, df[column_name] == filter_df["value"])
```

get table with unique count elements
from [stackoverflow](https://stackoverflow.com/questions/40888946/spark-dataframe-count-distinct-values-of-every-column)
```python
from pyspark.sql.functions import col, countDistinct

df.agg(*(countDistinct(col(c)).alias(c) for c in df.columns))
```

get Dummies on pyspark
from [stackoverflow](https://stackoverflow.com/questions/42805663/e-num-get-dummies-in-pySpark)
```python
pivot_cols = ['TYPE','CODE']
keys = ['ID','TYPE','CODE']

before = sc.parallelize([(1,'A','X1'),
                         (2,'B','X2'),
                         (3,'B','X3'),
                         (1,'B','X3'),
                         (2,'C','X2'),
                         (3,'C','X2'),
                         (1,'C','X1'),
                         (1,'B','X1')]).toDF(['ID','TYPE','CODE'])                         

#Helper function to recursively join a list of dataframes
#Can be simplified if you only need two columns
def join_all(dfs,keys):
    if len(dfs) > 1:
        return dfs[0].join(join_all(dfs[1:],keys), on = keys, how = 'inner')
    else:
        return dfs[0]

dfs = []
combined = []
for pivot_col in pivot_cols:
    pivotDF = before.groupBy(keys).pivot(pivot_col).count()
    new_names = pivotDF.columns[:len(keys)] +  ["e_{0}_{1}".format(pivot_col, c) for c in pivotDF.columns[len(keys):]]        
    df = pivotDF.toDF(*new_names).fillna(0)    
    combined.append(df)

join_all(combined,keys).show()
```

Split Time Series pySpark data frame into test & train without using random split
from [stackoverflow](https://stackoverflow.com/questions/51772908/split-time-series-pyspark-data-frame-into-test-train-without-using-random-spli)
```python
from pyspark.sql.functions import percent_rank
from pyspark.sql import Window

df = df.withColumn("rank", percent_rank().over(Window.partitionBy().orderBy("date")))
```


Count every element of unique values on pyspark column:
```python
df.groupBy('col_name').count().show()
```
[good guide](https://www.analyticsvidhya.com/blog/2019/11/build-machine-learning-pipelines-pyspark/)
[categ_guide](https://www.analyticsvidhya.com/blog/2015/11/easy-methods-deal-categorical-variables-predictive-modeling/?utm_source=blog&utm_medium=build-machine-learning-pipelines-pyspark)


Get dummies for pyspark:
```python
def get_dummies(dataframe, column_name):

    import pyspark.sql.functions as F
    
    list_values = [dataframe.select(column_name).distinct().collect()[i][0] for i in range(0, dataframe.select(column_name).distinct().count())]
    dummies_col = [F.when(F.col(column_name) == value, 1).otherwise(0).alias("{}_{}".format(column_name, value)) for value in list_values]
    
    return dataframe.select(column_name, *dummies_col)
```


Get dummies for pyspark **my version**:
```python
def pyspark_get_dummies(dataframe, columns_to_one_hot_encode):
    
    for pivot_col in columns_to_one_hot_encode:
        keys = dataframe.columns
        keys.remove(pivot_col)
        pivotDF = dataframe.groupBy(keys).pivot(pivot_col).count()
        pivoted_columns = pivotDF.columns
        added_columns = [i for i in pivoted_columns if i not in keys]
        added_columns

        pivotDF = pivotDF.fillna(0, subset=added_columns)

        for added_col in added_columns:
            pivotDF = pivotDF.withColumnRenamed(added_col,(pivot_col+"_"+added_col))
    
    return pivotDF
```

fill na for specific columns:
```python
df = df.fillna(0, subset=['colname_a', 'colname_b'])
```

Add column with count of zeros by row
```python
df.withColumn(
    "count",
    sum([
            F.when(F.col(cl) != 0, 1).otherwise(0) for cl in df.columns[1:]
    ])
).show()
```

Split a vector/list in a pyspark DataFrame into columns
```python
import pyspark.sql.functions as F

df2 = df.select([F.col("Col_name")[i] for i in df.columns])
df2.show()
```

Ways to get the max value of a column:
```python
# Method 1: Use describe()
float(df.describe("A").filter("summary = 'max'").select("A").collect()[0].asDict()['A'])

# Method 2: Use SQL
df.registerTempTable("df_table")
spark.sql("SELECT MAX(A) as maxval FROM df_table").collect()[0].asDict()['maxval']

# Method 3: Use groupby()
df.groupby().max('A').collect()[0].asDict()['max(A)']

# Method 4: Convert to RDD
df.select("A").rdd.max()[0]
```


group and operations on more than one column
from [stackoverflow](https://stackoverflow.com/questions/51632126/pysparkhow-to-calculate-avg-and-count-in-a-single-groupby)
```python
# Same column
from pyspark.sql import functions as F
df.groupBy("Profession").agg(F.mean('Age'), F.count('Age')).show()

# Different columns
df.groupBy("Profession").agg({'Age':'avg', 'Gender':'count'}).show()

(df
    .groupBy("order_item_order_id")
    .agg(func.col("order_item_order_id"), func.sum("order_item_subtotal"))
    .show())
```

melt pyspark data frame
from [stackoverflow](https://stackoverflow.com/questions/41670103/how-to-melt-spark-dataframe)
```python
from pyspark.sql.functions import array, col, explode, lit
from pyspark.sql.functions import create_map
from pyspark.sql import DataFrame
from typing import Iterable 
from itertools import chain

def melt(
        df: DataFrame, 
        id_vars: Iterable[str], value_vars: Iterable[str], 
        var_name: str="variable", value_name: str="value") -> DataFrame:
    """Convert :class:`DataFrame` from wide to long format."""

    # Create map<key: value>
    _vars_and_vals = create_map(
        list(chain.from_iterable([
            [lit(c), col(c)] for c in value_vars]
        ))
    )

    _tmp = df.select(*id_vars, explode(_vars_and_vals)) \
        .withColumnRenamed('key', var_name) \
        .withColumnRenamed('value', value_name)

    return _tmp
```

Sum all columns or list of columns from df by row
from [stackoverflow](https://stackoverflow.com/questions/66293025/row-sum-of-a-each-row-in-a-dataframe-using-pyspark)
```python
import pyspark.sql.functions as F

col_list = ['SUB1', 'SUB2', 'SUB3', 'SUB4']
# or col_list = df.columns[16:20] # Or df.columns or list of columns

df2 = df.withColumn(
    'SUM1',
    sum([F.col(c) for c in col_list])
)
```

Count number of null values by column on all columns
from [sparkbyexamples](https://sparkbyexamples.com/pyspark/pyspark-find-count-of-null-none-nan-values/)
```python
df.select([count(when(isnan(c) | col(c).isNull(), c)).alias(c) for c in df.columns]
   ).show()
```

Join DataFrames by specific key
from [sparkbyexamples](missing_reference)
```python
df_joined = (
    df1.join(
        df2,
        [
            df1.key1 == df2.key1_1,
            df1.key2 == df2.key2_1
        ],
        "left",
    )
    .drop(*['key1_1','key2_1'])
)
```

from array column to rows
from [sparkbyexamples](missing_reference)
```python
def list_column_to_row(df, column: str):
  """Transforms a column of list to rows"""
  columns = [x for x in df.columns if x != column]
  df = df.select(columns + [F.explode(column).alias(column)])
  return df
```

Sort data Frame
from [sparkbyexamples](https://sparkbyexamples.com/pyspark/pyspark-orderby-and-sort-explained/)
```python
df.sort(F.col("column_1"),F.col("column_2"))
```
