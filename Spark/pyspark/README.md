* [Compare two dataframes Pyspark](https://stackoverflow.com/questions/60279160/compare-two-dataframes-pyspark/60284807)
* [How to find count of Null and Nan values for each column in a PySpark dataframe efficiently?](https://stackoverflow.com/questions/44627386/how-to-find-count-of-null-and-nan-values-for-each-column-in-a-pyspark-dataframe)
* [Merge two DataFrames in PySpark](https://www.geeksforgeeks.org/merge-two-dataframes-in-pyspark/)
* [pyspark join types](https://sparkbyexamples.com/pyspark/pyspark-join-explained-with-examples/#pyspark-join-types)
* [pyspark: merge (outer-join) two data frames](https://stackoverflow.com/questions/38063657/pyspark-merge-outer-join-two-data-frames)
* [Merge two DataFrames in PySpark](https://www.geeksforgeeks.org/merge-two-dataframes-in-pyspark/)
* [dates with pyspark](https://mungingdata.com/apache-spark/dates-times/)
* [Working with dates and times in Spark](https://mrpowers.medium.com/working-with-dates-and-times-in-spark-491a9747a1d2)
* [PySpark Date Functions](https://sqlandhadoop.com/pyspark-date-functions/)
* [PySpark How to Filter Rows with NULL Values](https://sparkbyexamples.com/pyspark/pyspark-filter-rows-with-null-values/)
* [Sparkit-learn](https://github.com/lensacom/sparkit-learn)
* [pyspark split dataframe by rows](https://www.grepper.com/answers/371707/pyspark+split+dataframe+by+rows)
* [Fill NAs with mode of column based on aggregation of other columns](https://stackoverflow.com/questions/65928710/pyspark-fill-nas-with-mode-of-column-based-on-aggregation-of-other-columns)
* [PySpark -> interpolate values in one column](https://stackoverflow.com/questions/57114835/pyspark-interpolate-values-in-one-column)
* [HandySpark: bringing pandas-like capabilities to Spark DataFrames](https://towardsdatascience.com/handyspark-bringing-pandas-like-capabilities-to-spark-dataframes-5f1bcea9039e)
* [The Most Complete Guide to pySpark DataFrames](https://towardsdatascience.com/the-most-complete-guide-to-pyspark-dataframes-2702c343b2e8)
* [Apache Spark Performance Boosting](https://towardsdatascience.com/apache-spark-performance-boosting-e072a3ec1179)
* [Using PySpark Dataframe as in Python](https://medium.com/@zhiqiangzhong/using-pyspark-dataframe-as-python-dataframe-2959c2a085e)
* [Working with PySpark DataFrames](https://medium.com/@aniketmohan/working-with-pyspark-dataframes-456c7bdb5b5c)
* [Pyspark dataframe: Summing over a column while grouping over another](https://stackoverflow.com/questions/33961899/pyspark-dataframe-summing-over-a-column-while-grouping-over-another)
* [PySpark Add a New Column to DataFrame](https://sparkbyexamples.com/pyspark/pyspark-add-new-column-to-dataframe/)
* [PySpark Groupby Explained with Example](https://sparkbyexamples.com/pyspark/pyspark-groupby-explained-with-example/)
* [Pyspark: GroupBy and Aggregate Functions](https://hendra-herviawan.github.io/pyspark-groupby-and-aggregate-functions.html)
* [Extracting, transforming and selecting features](https://spark.apache.org/docs/3.0.0-preview/ml-features.html)
* [Basic data preparation in Pyspark — Capping, Normalizing and Scaling](https://medium.com/@connectwithghosh/basic-data-preparation-in-pyspark-capping-normalizing-and-scaling-252ee7acba7d)
* [GET DATA TYPE OF COLUMN IN PYSPARK (SINGLE & MULTIPLE COLUMNS)](https://www.datasciencemadesimple.com/data-type-of-column-in-pyspark-single-multiple-columns/)
* [get datatype of column using pyspark](https://stackoverflow.com/questions/45033315/get-datatype-of-column-using-pyspark)
* [PySpark: withColumn() with two conditions and three outcomes](https://stackoverflow.com/questions/40161879/pyspark-withcolumn-with-two-conditions-and-three-outcomes)
* [ADD LEADING ZEROS TO THE COLUMN IN PYSPARK](https://www.datasciencemadesimple.com/add-leading-zeros-to-the-column-in-pyspark/)
* [PySpark Read and Write Parquet File](https://sparkbyexamples.com/pyspark/pyspark-read-and-write-parquet-file/)
* [PySpark Replace Column Values in DataFrame](https://sparkbyexamples.com/pyspark/pyspark-replace-column-values/)
* [PySpark When Otherwise | SQL Case When Usage](https://sparkbyexamples.com/pyspark/pyspark-when-otherwise/)
* [PySpark Where Filter Function | Multiple Conditions](https://sparkbyexamples.com/pyspark/pyspark-where-filter/)
* [Show distinct column values in pyspark dataframe](https://stackoverflow.com/questions/39383557/show-distinct-column-values-in-pyspark-dataframe)
* [PySpark orderBy() and sort() explained](https://sparkbyexamples.com/pyspark/pyspark-orderby-and-sort-explained/)
* [PySpark fillna() & fill() – Replace NULL/None Values](https://sparkbyexamples.com/pyspark/pyspark-fillna-fill-replace-null-values/)
* [PySpark Concatenate Columns](https://sparkbyexamples.com/pyspark/pyspark-concatenate-columns/)
* [ML Pipelines](https://spark.apache.org/docs/latest/ml-pipeline.html)
* [Overview: estimators, transformers and pipelines - spark.ml](https://spark.apache.org/docs/1.6.1/ml-guide.html)
* [pyspark ml pipeline](https://www.programcreek.com/python/example/105437/pyspark.ml.Pipeline)
* []()
* []()
* [Plot Data from Apache Spark in Python/v3](https://plotly.com/python/v3/apache-spark/)
* [PySpark apply function to column](https://sqlandhadoop.com/pyspark-apply-function-to-column/)
* [PySpark SQL expr() (Expression ) Function](https://sparkbyexamples.com/pyspark/pyspark-sql-expr-expression-function/)
* [PySpark lit() – Add Literal or Constant to DataFrame](https://sparkbyexamples.com/pyspark/pyspark-lit-add-literal-constant/)
* [PySpark map() Transformation](https://sparkbyexamples.com/pyspark/pyspark-map-transformation/)
* [PySpark flatMap() Transformation](https://sparkbyexamples.com/pyspark/pyspark-flatmap-transformation/)
* [PySpark MapType (Dict) Usage with Examples](https://sparkbyexamples.com/pyspark/pyspark-maptype-dict-examples/)
* [PySpark Convert DataFrame Columns to MapType (Dict)](https://sparkbyexamples.com/pyspark/pyspark-convert-dataframe-columns-to-maptype-dict/#:~:text=Solution%3A%20PySpark%20SQL%20function%20create_map,and%20returns%20a%20MapType%20column.)
* [Pyspark apply function to column value if condition is met [duplicate]](https://stackoverflow.com/questions/56309596/pyspark-apply-function-to-column-value-if-condition-is-met)
* [Pyspark: Pass multiple columns in UDF](https://stackoverflow.com/questions/42540169/pyspark-pass-multiple-columns-in-udf)
* [How To Select Rows From PySpark DataFrames Based on Column Values](https://towardsdatascience.com/select-rows-pyspark-df-based-on-column-values-3146afe4dee3)
* [When Otherwise | SQL Case When Usage](https://sparkbyexamples.com/pyspark/pyspark-when-otherwise/)
* [PySpark & Plotly](https://nnovakova.github.io/pyspark-plotly/)
* [orderBy() and sort() explained](https://sparkbyexamples.com/pyspark/pyspark-orderby-and-sort-explained/)
* [cartesian product (filling spaces](https://stackoverflow.com/questions/40728618/creating-each-row-for-each-item-for-a-user-in-spark-dataframe)
* [The Most Complete Guide to pySpark DataFrames](https://towardsdatascience.com/the-most-complete-guide-to-pyspark-dataframes-2702c343b2e8)
* [9 most useful functions for PySpark DataFrame](https://www.analyticsvidhya.com/blog/2021/05/9-most-useful-functions-for-pyspark-dataframe/)
* [Pyspark Data Manipulation Tutorial](https://towardsdatascience.com/pyspark-data-manipulation-tutorial-8c62652f35fa)
* [PySpark Where Filter Function | Multiple Conditions](https://sparkbyexamples.com/pyspark/pyspark-where-filter/)
* [PySpark split() Column into Multiple Columns](https://sparkbyexamples.com/pyspark/pyspark-split-dataframe-column-into-multiple-columns/)
* [Sum a column in dataframe and return results as int](https://stackoverflow.com/questions/47812526/pyspark-sum-a-column-in-dataframe-and-return-results-as-int)
* [Extracting, transforming and selecting features](https://spark.apache.org/docs/latest/ml-features)
* [Feature Engineering in pyspark](https://dhiraj-p-rai.medium.com/essentials-of-feature-engineering-in-pyspark-part-i-76a57680a85)
* [Get specific row from PySpark dataframe](https://www.geeksforgeeks.org/get-specific-row-from-pyspark-dataframe/)

drop column by name
```python
df.drop(col("firstname"))
```
drop rows with null values
from [stackOverflow](https://sparkbyexamples.com/pyspark/pyspark-drop-rows-with-null-values/)
```python
df.na.drop("any").show(false)
```

list of unique values from column:
```python
unique_list = [i.variable for i in df.select('variable').distinct().collect()]
```


filter df with elements of dict:
```python
filtered_df = df.filter(F.col('col_name').isin(dict_name['dict_element_name']))
```

filter df with elements of list:
```python
filtered_df = df.filter(F.col('col_name').isin(list_name))
```
or
```python
filtered_df.where((filtered_df.col_name).isin(list_name))
```


Join 2 df's:
guide is [here](https://sparkbyexamples.com/pyspark/pyspark-join-explained-with-examples/)
```python
df_joined = df_orig.join(df_to_join, on=join_cols, how='left')
```


renaming a column:
```python
df = df.withColumnRenamed(column_name, new_column_name)
```

copy a column:
```python
df = df.withColumn("new_column_name", df["column_name"])
```

change columns data type:
```python
df = df.withColumn("column_name", df["column_name"].cast(IntegerType()))
```

select columns:
```python
df_selected_cols = df_original.select('cols_to_select')
```

get columns data types:
```python
df.dtypes
```

get elements type on list:
```python
from collections import Counter

Counter([type(value) for value in lista])
```

delete NoneType element from list:
```python
res = list(filter(None, lista))
```

convert pyspark DF to Pandas DF:
```python
pandas_df = pyspark_df.toPandas()
```

convert Pandas DF to Pyspark DF:
```python
sparkDF=spark.createDataFrame(pandasDF) 
```

add a column with value depending of whether the value is in list or not

```python
prueba = (
    df
    .withColumn("new_col_name",sf.when(sf.col(col_name).isin(list_name),"value to assign").otherwise("value if not true") )
) 
```

or

```python
prueba = (
    df
    .withColumn("new_col_name",sf.when(sf.col(col_name).isin(list_name),"value to assign").otherwise(sf.col(col_name) )
) 
```

Filter by column value
```python
df.filter(sf.col('column_name')==value_looking_for)
```

from [stackoverflow](https://stackoverflow.com/questions/40421845/pyspark-dataframe-filter-or-include-based-on-list)
```python
def filter_spark_dataframe_by_list(df, column_name, filter_list):
    """ Returns subset of df where df[column_name] is in filter_list """
    spark = SparkSession.builder.getOrCreate()
    filter_df = spark.createDataFrame(filter_list, df.schema[column_name].dataType)
    return df.join(filter_df, df[column_name] == filter_df["value"])
```

get table with unique count elements
from [stackoverflow](https://stackoverflow.com/questions/40888946/spark-dataframe-count-distinct-values-of-every-column)
```python
from pyspark.sql.functions import col, countDistinct

df.agg(*(countDistinct(col(c)).alias(c) for c in df.columns))
```

get Dummies on pyspark
from [stackoverflow](https://stackoverflow.com/questions/42805663/e-num-get-dummies-in-pySpark)
```python
pivot_cols = ['TYPE','CODE']
keys = ['ID','TYPE','CODE']

before = sc.parallelize([(1,'A','X1'),
                         (2,'B','X2'),
                         (3,'B','X3'),
                         (1,'B','X3'),
                         (2,'C','X2'),
                         (3,'C','X2'),
                         (1,'C','X1'),
                         (1,'B','X1')]).toDF(['ID','TYPE','CODE'])                         

#Helper function to recursively join a list of dataframes
#Can be simplified if you only need two columns
def join_all(dfs,keys):
    if len(dfs) > 1:
        return dfs[0].join(join_all(dfs[1:],keys), on = keys, how = 'inner')
    else:
        return dfs[0]

dfs = []
combined = []
for pivot_col in pivot_cols:
    pivotDF = before.groupBy(keys).pivot(pivot_col).count()
    new_names = pivotDF.columns[:len(keys)] +  ["e_{0}_{1}".format(pivot_col, c) for c in pivotDF.columns[len(keys):]]        
    df = pivotDF.toDF(*new_names).fillna(0)    
    combined.append(df)

join_all(combined,keys).show()
```

Split Time Series pySpark data frame into test & train without using random split
from [stackoverflow](https://stackoverflow.com/questions/51772908/split-time-series-pyspark-data-frame-into-test-train-without-using-random-spli)
```python
from pyspark.sql.functions import percent_rank
from pyspark.sql import Window

df = df.withColumn("rank", percent_rank().over(Window.partitionBy().orderBy("date")))
```


Count every element of unique values on pyspark column:
```python
df.groupBy('col_name').count().show()
```
[good guide](https://www.analyticsvidhya.com/blog/2019/11/build-machine-learning-pipelines-pyspark/)
[categ_guide](https://www.analyticsvidhya.com/blog/2015/11/easy-methods-deal-categorical-variables-predictive-modeling/?utm_source=blog&utm_medium=build-machine-learning-pipelines-pyspark)


Get dummies for pyspark:
```python
def get_dummies(dataframe, column_name):

    import pyspark.sql.functions as F
    
    list_values = [dataframe.select(column_name).distinct().collect()[i][0] for i in range(0, dataframe.select(column_name).distinct().count())]
    dummies_col = [F.when(F.col(column_name) == value, 1).otherwise(0).alias("{}_{}".format(column_name, value)) for value in list_values]
    
    return dataframe.select(column_name, *dummies_col)
```


Get dummies for pyspark **my version**:
```python
def pyspark_get_dummies(dataframe, columns_to_one_hot_encode):
    
    for pivot_col in columns_to_one_hot_encode:
        keys = dataframe.columns
        keys.remove(pivot_col)
        pivotDF = dataframe.groupBy(keys).pivot(pivot_col).count()
        pivoted_columns = pivotDF.columns
        added_columns = [i for i in pivoted_columns if i not in keys]
        added_columns

        pivotDF = pivotDF.fillna(0, subset=added_columns)

        for added_col in added_columns:
            pivotDF = pivotDF.withColumnRenamed(added_col,(pivot_col+"_"+added_col))
    
    return pivotDF
```

fill na for specific columns:
```python
df = df.fillna(0, subset=['colname_a', 'colname_b'])
```

Add column with count of zeros by row
```python
df.withColumn(
    "count",
    sum([
            F.when(F.col(cl) != 0, 1).otherwise(0) for cl in df.columns[1:]
    ])
).show()
```

Split a vector/list in a pyspark DataFrame into columns
```python
import pyspark.sql.functions as F

df2 = df.select([F.col("Col_name")[i] for i in df.columns])
df2.show()
```

Ways to get the max value of a column:
```python
# Method 1: Use describe()
float(df.describe("A").filter("summary = 'max'").select("A").collect()[0].asDict()['A'])

# Method 2: Use SQL
df.registerTempTable("df_table")
spark.sql("SELECT MAX(A) as maxval FROM df_table").collect()[0].asDict()['maxval']

# Method 3: Use groupby()
df.groupby().max('A').collect()[0].asDict()['max(A)']

# Method 4: Convert to RDD
df.select("A").rdd.max()[0]
```


group and operations on more than one column
from [stackoverflow](https://stackoverflow.com/questions/51632126/pysparkhow-to-calculate-avg-and-count-in-a-single-groupby)
```python
# Same column
from pyspark.sql import functions as F
df.groupBy("Profession").agg(F.mean('Age'), F.count('Age')).show()

# Different columns
df.groupBy("Profession").agg({'Age':'avg', 'Gender':'count'}).show()

(df
    .groupBy("order_item_order_id")
    .agg(func.col("order_item_order_id"), func.sum("order_item_subtotal"))
    .show())
```

melt pyspark data frame
from [stackoverflow](https://stackoverflow.com/questions/41670103/how-to-melt-spark-dataframe)
```python
from pyspark.sql.functions import array, col, explode, lit
from pyspark.sql.functions import create_map
from pyspark.sql import DataFrame
from typing import Iterable 
from itertools import chain

def melt(
        df: DataFrame, 
        id_vars: Iterable[str], value_vars: Iterable[str], 
        var_name: str="variable", value_name: str="value") -> DataFrame:
    """Convert :class:`DataFrame` from wide to long format."""

    # Create map<key: value>
    _vars_and_vals = create_map(
        list(chain.from_iterable([
            [lit(c), col(c)] for c in value_vars]
        ))
    )

    _tmp = df.select(*id_vars, explode(_vars_and_vals)) \
        .withColumnRenamed('key', var_name) \
        .withColumnRenamed('value', value_name)

    return _tmp
```

Sum all columns or list of columns from df by row
from [stackoverflow](https://stackoverflow.com/questions/66293025/row-sum-of-a-each-row-in-a-dataframe-using-pyspark)
```python
import pyspark.sql.functions as F

col_list = ['SUB1', 'SUB2', 'SUB3', 'SUB4']
# or col_list = df.columns[16:20] # Or df.columns or list of columns

df2 = df.withColumn(
    'SUM1',
    sum([F.col(c) for c in col_list])
)
```

Count number of null values by column on all columns
from [sparkbyexamples](https://sparkbyexamples.com/pyspark/pyspark-find-count-of-null-none-nan-values/)
```python
df.select([count(when(isnan(c) | col(c).isNull(), c)).alias(c) for c in df.columns]
   ).show()
```

Join DataFrames by specific key
from [sparkbyexamples](missing_reference)
```python
df_joined = (
    df1.join(
        df2,
        [
            df1.key1 == df2.key1_1,
            df1.key2 == df2.key2_1
        ],
        "left",
    )
    .drop(*['key1_1','key2_1'])
)
```

Sort data Frame
from [sparkbyexamples](https://sparkbyexamples.com/pyspark/pyspark-orderby-and-sort-explained/)
```python
df.sort(F.col("column_1"),F.col("column_2"))
```
